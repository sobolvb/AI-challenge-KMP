package com.aichallengekmp.ai

import io.ktor.client.*
import io.ktor.client.call.*
import io.ktor.client.plugins.*
import io.ktor.client.request.*
import io.ktor.client.statement.*
import io.ktor.http.*
import kotlinx.serialization.SerialName
import kotlinx.serialization.Serializable
import kotlinx.serialization.json.*
import org.slf4j.LoggerFactory

/**
 * –ü—Ä–æ–≤–∞–π–¥–µ—Ä –¥–ª—è Ollama (–ª–æ–∫–∞–ª—å–Ω—ã–µ LLM –º–æ–¥–µ–ª–∏)
 *
 * –ü–æ–¥–¥–µ—Ä–∂–∫–∞:
 * - Qwen2.5, Llama, Mistral –∏ –¥—Ä—É–≥–∏–µ –º–æ–¥–µ–ª–∏ —á–µ—Ä–µ–∑ Ollama
 * - API: http://localhost:11434/api/chat
 * - –ù–ï –ø–æ–¥–¥–µ—Ä–∂–∏–≤–∞–µ—Ç function calling (–ø–æ–∫–∞)
 */
class OllamaProvider(
    private val httpClient: HttpClient,
    private val baseUrl: String = "http://localhost:11434"
) : AIProvider {

    private val logger = LoggerFactory.getLogger(OllamaProvider::class.java)

    override val providerId = "ollama"

    companion object {
        // –î–æ—Å—Ç—É–ø–Ω—ã–µ –º–æ–¥–µ–ª–∏ Ollama
        // –¢–æ–ª—å–∫–æ —Ä–µ–∞–ª—å–Ω–æ —É—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω–Ω—ã–µ –º–æ–¥–µ–ª–∏ (–ø—Ä–æ–≤–µ—Ä—è–µ—Ç—Å—è —á–µ—Ä–µ–∑ `ollama list`)
        private val MODELS = listOf(
            AIModel(
                id = "qwen2.5:14b",
                name = "Qwen 2.5 14B",
                displayName = "Qwen 2.5 14B (–ª–æ–∫–∞–ª—å–Ω–∞—è)",
                providerId = "ollama",
                maxTokens = 32768,
                supportsSystemPrompt = true
            )
        )
    }

    override suspend fun getSupportedModels(): List<AIModel> {
        logger.debug("üìã –ó–∞–ø—Ä–æ—Å —Å–ø–∏—Å–∫–∞ –ø–æ–¥–¥–µ—Ä–∂–∏–≤–∞–µ–º—ã—Ö –º–æ–¥–µ–ª–µ–π Ollama")

        // –û–ø—Ü–∏–æ–Ω–∞–ª—å–Ω–æ: –º–æ–∂–Ω–æ –∑–∞–ø—Ä–æ—Å–∏—Ç—å —Ä–µ–∞–ª—å–Ω–æ —É—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω–Ω—ã–µ –º–æ–¥–µ–ª–∏ —á–µ—Ä–µ–∑ /api/tags
        // –ü–æ–∫–∞ –≤–æ–∑–≤—Ä–∞—â–∞–µ–º —Å—Ç–∞—Ç–∏—á–µ—Å–∫–∏–π —Å–ø–∏—Å–æ–∫
        return MODELS
    }

    override suspend fun complete(request: CompletionRequest): CompletionResult {
        // Ollama –ø–æ–∫–∞ –Ω–µ –ø–æ–¥–¥–µ—Ä–∂–∏–≤–∞–µ—Ç function calling –≤ –Ω–∞—à–µ–π —Ä–µ–∞–ª–∏–∑–∞—Ü–∏–∏
        if (!request.tools.isNullOrEmpty()) {
            logger.warn("‚ö†Ô∏è Ollama –Ω–µ –ø–æ–¥–¥–µ—Ä–∂–∏–≤–∞–µ—Ç function calling, –∏–≥–Ω–æ—Ä–∏—Ä—É–µ–º tools")
        }

        return completeChat(request)
    }

    /**
     * –ó–∞–ø—Ä–æ—Å —á–µ—Ä–µ–∑ Ollama Chat API
     */
    private suspend fun completeChat(request: CompletionRequest): CompletionResult {
        logger.info("ü§ñ –û—Ç–ø—Ä–∞–≤–∫–∞ –∑–∞–ø—Ä–æ—Å–∞ –≤ Ollama")
        logger.debug("   –ú–æ–¥–µ–ª—å: ${request.modelId}")
        logger.debug("   –¢–µ–º–ø–µ—Ä–∞—Ç—É—Ä–∞: ${request.temperature}")
        logger.debug("   –°–æ–æ–±—â–µ–Ω–∏–π: ${request.messages.size}")

        // –§–æ—Ä–º–∏—Ä—É–µ–º —Å–æ–æ–±—â–µ–Ω–∏—è –¥–ª—è Ollama API (—Ñ–æ—Ä–º–∞—Ç OpenAI-—Å–æ–≤–º–µ—Å—Ç–∏–º—ã–π)
        val messages = buildMessages(request)

        val requestBody = OllamaChatRequest(
            model = request.modelId,
            messages = messages,
            options = OllamaOptions(
                temperature = request.temperature,
                num_predict = request.maxTokens,
                top_p = request.topP,
                top_k = request.topK,
                num_ctx = request.numCtx,
                repeat_penalty = request.repeatPenalty,
                seed = request.seed
            ),
            stream = false
        )

        logger.debug("üì§ –û—Ç–ø—Ä–∞–≤–∫–∞ –∑–∞–ø—Ä–æ—Å–∞ –≤ Ollama API: $baseUrl/api/chat")

        val rawResponse = try {
            httpClient.post("$baseUrl/api/chat") {
                contentType(ContentType.Application.Json)
                setBody(requestBody)
                // –£–≤–µ–ª–∏—á–µ–Ω–Ω—ã–π timeout –¥–ª—è –ø–µ—Ä–≤–æ–π –∑–∞–≥—Ä—É–∑–∫–∏ –º–æ–¥–µ–ª–∏ –≤ –ø–∞–º—è—Ç—å (–º–æ–∂–µ—Ç –∑–∞–Ω—è—Ç—å 30-60 —Å–µ–∫—É–Ω–¥)
                this.timeout {
                    requestTimeoutMillis = 120_000  // 2 –º–∏–Ω—É—Ç—ã
                }
            }.bodyAsText()
        } catch (e: Exception) {
            logger.error("‚ùå –û—à–∏–±–∫–∞ –ø—Ä–∏ –≤—ã–∑–æ–≤–µ Ollama API: ${e.message}", e)
            throw AIProviderException("–û—à–∏–±–∫–∞ –ø—Ä–∏ –æ–±—Ä–∞—â–µ–Ω–∏–∏ –∫ Ollama: ${e.message}. –£–±–µ–¥–∏—Ç–µ—Å—å —á—Ç–æ Ollama –∑–∞–ø—É—â–µ–Ω (ollama serve)", e)
        }

        logger.debug("RAW OLLAMA RESPONSE (–ø–µ—Ä–≤—ã–µ 500 —Å–∏–º–≤–æ–ª–æ–≤) = ${rawResponse.take(500)}")

        // Ollama –≤–æ–∑–≤—Ä–∞—â–∞–µ—Ç streaming response –ø–æ—Å—Ç—Ä–æ—á–Ω–æ, –¥–∞–∂–µ –ø—Ä–∏ stream=false
        // –ù—É–∂–Ω–æ –ø–∞—Ä—Å–∏—Ç—å –∫–∞–∂–¥—É—é —Å—Ç—Ä–æ–∫—É –∏ —Å–æ–±—Ä–∞—Ç—å –ø–æ–ª–Ω—ã–π –æ—Ç–≤–µ—Ç
        val json = Json { ignoreUnknownKeys = true }
        val fullText = StringBuilder()
        var finalResponse: OllamaChatResponse? = null

        try {
            rawResponse.lines().forEach { line ->
                if (line.isBlank()) return@forEach

                val chunk = json.decodeFromString<OllamaChatResponse>(line)

                // –°–æ–±–∏—Ä–∞–µ–º —Ç–µ–∫—Å—Ç –∏–∑ –∫–∞–∂–¥–æ–≥–æ —á–∞–Ω–∫–∞
                chunk.message.content?.let { fullText.append(it) }

                // –ü–æ—Å–ª–µ–¥–Ω–∏–π —á–∞–Ω–∫ —Å–æ–¥–µ—Ä–∂–∏—Ç —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫—É —Ç–æ–∫–µ–Ω–æ–≤
                if (chunk.done) {
                    finalResponse = chunk
                }
            }
        } catch (e: Exception) {
            logger.error("‚ùå –û—à–∏–±–∫–∞ –ø–∞—Ä—Å–∏–Ω–≥–∞ streaming –æ—Ç–≤–µ—Ç–∞ Ollama: ${e.message}", e)
            throw AIProviderException("–û—à–∏–±–∫–∞ –ø—Ä–∏ —Ä–∞–∑–±–æ—Ä–µ –æ—Ç–≤–µ—Ç–∞ Ollama: ${e.message}", e)
        }

        val text = fullText.toString().takeIf { it.isNotBlank() }
            ?: throw AIProviderException("–ü—É—Å—Ç–æ–π –æ—Ç–≤–µ—Ç –æ—Ç Ollama")

        // Ollama –≤–æ–∑–≤—Ä–∞—â–∞–µ—Ç —Ç–æ–∫–µ–Ω—ã –≤ —Ñ–∏–Ω–∞–ª—å–Ω–æ–º —á–∞–Ω–∫–µ
        val tokenUsage = TokenUsage(
            inputTokens = finalResponse?.prompt_eval_count ?: 0,
            outputTokens = finalResponse?.eval_count ?: 0,
            totalTokens = (finalResponse?.prompt_eval_count ?: 0) + (finalResponse?.eval_count ?: 0)
        )

        logger.info("‚úÖ –ü–æ–ª—É—á–µ–Ω –æ—Ç–≤–µ—Ç –æ—Ç Ollama")
        logger.debug("   –¢–æ–∫–µ–Ω—ã: input=${tokenUsage.inputTokens}, output=${tokenUsage.outputTokens}, total=${tokenUsage.totalTokens}")

        return CompletionResult(
            text = text,
            modelId = request.modelId,
            tokenUsage = tokenUsage
        )
    }

    /**
     * –§–æ—Ä–º–∏—Ä—É–µ–º —Å–æ–æ–±—â–µ–Ω–∏—è –¥–ª—è Ollama (OpenAI-compatible format)
     */
    private fun buildMessages(request: CompletionRequest): List<OllamaMessage> {
        val messages = mutableListOf<OllamaMessage>()

        // System prompt –∏–¥–µ—Ç –ø–µ—Ä–≤—ã–º —Å–æ–æ–±—â–µ–Ω–∏–µ–º
        request.systemPrompt?.takeIf { it.isNotBlank() }?.let {
            messages += OllamaMessage(role = "system", content = it)
        }

        // –î–æ–±–∞–≤–ª—è–µ–º –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—å—Å–∫–∏–µ —Å–æ–æ–±—â–µ–Ω–∏—è
        request.messages.forEach { msg ->
            // –ü—Ä–æ–ø—É—Å–∫–∞–µ–º —Å–æ–æ–±—â–µ–Ω–∏—è —Å tool calls/results (Ollama –∏—Ö –Ω–µ –ø–æ–¥–¥–µ—Ä–∂–∏–≤–∞–µ—Ç)
            if (msg.toolCallList != null || msg.toolResultList != null) {
                logger.debug("‚è≠Ô∏è –ü—Ä–æ–ø—É—Å–∫–∞–µ–º —Å–æ–æ–±—â–µ–Ω–∏–µ —Å tool calls/results")
                return@forEach
            }

            val content = msg.content?.takeIf { it.isNotBlank() } ?: return@forEach

            messages += OllamaMessage(
                role = msg.role,
                content = content
            )
        }

        return messages
    }

    // ============= Ollama API Models =============

    @Serializable
    private data class OllamaChatRequest(
        val model: String,
        val messages: List<OllamaMessage>,
        val options: OllamaOptions? = null,
        val stream: Boolean = false
    )

    @Serializable
    private data class OllamaMessage(
        val role: String,
        val content: String
    )

    @Serializable
    private data class OllamaOptions(
        val temperature: Double,
        val num_predict: Int? = null,       // max tokens
        val top_p: Double? = null,          // nucleus sampling
        val top_k: Int? = null,             // top-k sampling
        val num_ctx: Int? = null,           // context window
        val repeat_penalty: Double? = null, // repeat penalty
        val seed: Int? = null               // random seed
    )

    @Serializable
    private data class OllamaChatResponse(
        val model: String,
        val message: OllamaMessage,
        @SerialName("created_at") val createdAt: String,
        val done: Boolean,

        // Token usage stats
        @SerialName("total_duration") val total_duration: Long? = null,
        @SerialName("load_duration") val load_duration: Long? = null,
        @SerialName("prompt_eval_count") val prompt_eval_count: Int? = null,
        @SerialName("eval_count") val eval_count: Int? = null,
        @SerialName("eval_duration") val eval_duration: Long? = null
    )
}
