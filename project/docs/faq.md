# FAQ - Часто задаваемые вопросы

## Авторизация и безопасность

### Почему не работает авторизация?

Проблемы с авторизацией могут возникать по следующим причинам:

1. **Неправильные учетные данные** - проверьте логин и пароль
2. **Истек токен доступа** - перелогиньтесь в системе
3. **Не настроены переменные окружения** - убедитесь, что указаны GITHUB_TOKEN, YANDEX_API_KEY, YANDEX_FOLDER_ID
4. **Проблемы с сессией** - очистите cookies и кэш браузера

### Как сбросить пароль?

На данный момент сброс пароля не реализован в системе. Обратитесь к администратору для восстановления доступа.

### Безопасно ли хранятся мои данные?

Все данные шифруются при передаче через HTTPS. Токены доступа хранятся в зашифрованном виде. Пароли хешируются с использованием bcrypt.

## Чат и AI модели

### Какие AI модели доступны?

Система поддерживает следующие модели:
- **YandexGPT Lite** - быстрая модель для простых запросов
- **Ollama (llama3.2)** - локальная модель для офлайн работы
- **Ollama (qwen2.5-coder)** - специализированная модель для кода

### Почему чат не отвечает?

Возможные причины:
1. **Проблемы с сетью** - проверьте интернет-соединение
2. **Сервер недоступен** - убедитесь, что backend запущен на порту 8080
3. **Превышен лимит токенов** - сократите размер запроса
4. **Ошибка API ключа** - проверьте корректность YANDEX_API_KEY

### Как переключиться между моделями?

В интерфейсе чата выберите нужную модель из выпадающего списка в верхней части экрана. Выбранная модель будет использоваться для всех последующих сообщений.

### Что делать, если модель выдает некорректные ответы?

1. Попробуйте переформулировать вопрос более четко
2. Проверьте, подходит ли модель для вашей задачи (coding vs general)
3. Увеличьте или уменьшите temperature в настройках
4. Попробуйте другую модель

## RAG (Retrieval Augmented Generation)

### Что такое RAG и зачем он нужен?

RAG - это технология, которая позволяет AI модели использовать дополнительный контекст из вашей документации и кода. Это повышает точность ответов и позволяет работать с актуальной информацией о вашем проекте.

### Как проиндексировать документацию?

Используйте API endpoint:
```bash
curl -X POST http://localhost:8080/api/rag/index/docs
```

Это автоматически проиндексирует все .md файлы из директории project/docs.

### Как проиндексировать код проекта?

Используйте API endpoint:
```bash
curl -X POST http://localhost:8080/api/rag/index/code
```

Это проиндексирует все .kt файлы из src/main/kotlin и shared/src/commonMain/kotlin.

### Почему поиск не находит мою документацию?

Возможные причины:
1. **Документация не проиндексирована** - запустите индексацию через /api/rag/index/docs
2. **Ollama не запущен** - убедитесь, что Ollama работает на порту 11434
3. **Неподходящий поисковый запрос** - попробуйте использовать другие ключевые слова
4. **Низкий порог similarity** - релевантность чанка может быть ниже минимального значения

## MCP (Model Context Protocol)

### Что такое MCP серверы?

MCP серверы - это инструменты, которые предоставляют AI модели доступ к внешним системам: git репозиторию, файловой системе, базам данных, API и т.д.

### Какие MCP серверы доступны?

- **/mcp/filesystem** - работа с файловой системой (чтение, запись, поиск)
- **/mcp/git** - операции с git (PR diff, список файлов, история)
- **/mcp/tracker** - трекер задач (создание, поиск, обновление задач)
- **/mcp/support** - система поддержки (пользователи, тикеты)

### Как использовать MCP в чате?

MCP серверы используются автоматически, когда AI модели нужен доступ к определенным данным. Вы просто задаете вопрос, а система сама выбирает нужные инструменты.

### Почему MCP инструмент не вызывается?

1. **Модель не поддерживает function calling** - используйте YandexGPT или Ollama с поддержкой tools
2. **Неправильный промпт** - сформулируйте запрос более явно (например, "прочитай файл X")
3. **Ошибка в MCP сервере** - проверьте логи сервера

## Производительность

### Почему система работает медленно?

Возможные причины:
1. **Ollama долго генерирует embeddings** - добавлены задержки между запросами для стабильности
2. **Большой объем данных для индексации** - индексация может занимать время
3. **Медленное интернет-соединение** - влияет на работу с YandexGPT
4. **Ограничения железа** - локальные модели Ollama требуют ресурсов

### Как ускорить индексацию?

1. Используйте более быстрые embedding модели
2. Уменьшите размер чанков (меньше строк на чанк)
3. Исключите ненужные файлы из индексации
4. Увеличьте задержку между запросами если Ollama перегружен

## Code Review

### Как работает автоматическое ревью кода?

При создании Pull Request автоматически запускается GitHub Action, который:
1. Получает diff и метаданные PR через MCP
2. Ищет релевантный код и style guidelines через RAG
3. Отправляет все данные в YandexGPT для анализа
4. Публикует результаты как комментарий к PR

### Почему GitHub Action не создает комментарий?

Проверьте:
1. **Permissions в workflow** - должны быть issues: write и pull-requests: write
2. **CODE_REVIEW_SERVER_URL в secrets** - должен указывать на ngrok URL
3. **Сервер запущен** - backend должен быть доступен через ngrok
4. **Переменные окружения** - YANDEX_API_KEY и YANDEX_FOLDER_ID должны быть настроены

### Можно ли настроить правила ревью?

Да, отредактируйте системный промпт в CodeReviewService.kt, добавив свои специфические правила проверки кода.

## Интеграция и развертывание

### Как запустить проект локально?

1. Установите Ollama и загрузите модели (llama3.2, qwen2.5-coder, nomic-embed-text)
2. Настройте переменные окружения в IDE Run Configuration
3. Запустите backend: `./gradlew :server:run`
4. Frontend запустится автоматически на http://localhost:8080

### Нужен ли интернет для работы?

Частично. Ollama модели работают офлайн, но YandexGPT требует интернет. RAG может работать с локальными embeddings через Ollama.

### Как настроить ngrok для GitHub Actions?

1. Установите ngrok: `brew install ngrok`
2. Запустите: `ngrok http 8080`
3. Скопируйте HTTPS URL
4. Добавьте в GitHub secrets как CODE_REVIEW_SERVER_URL

## Ошибки и отладка

### Ошибка "Модель не найдена"

Проверьте, что:
1. Модель зарегистрирована в ModelRegistry
2. Для YandexGPT используйте modelId: "yandexgpt-lite"
3. Для Ollama убедитесь, что модель загружена: `ollama list`

### Ошибка "EOF from Ollama"

Ollama перегружен запросами. Решение:
1. Увеличьте задержки между запросами
2. Добавьте retry логику с exponential backoff
3. Проверьте доступность Ollama: `curl http://localhost:11434`

### Ошибка "403 Resource not accessible"

GitHub Action не имеет прав. Добавьте в workflow:
```yaml
permissions:
  issues: write
  pull-requests: write
  contents: read
```

### Где смотреть логи?

Логи сервера выводятся в консоль IDE. Уровень логирования настраивается в logback.xml. Для GitHub Actions логи доступны во вкладке Actions в репозитории.

## Поддержка

### Как связаться с технической поддержкой?

1. Создайте issue в GitHub репозитории
2. Напишите на support@example.com
3. Используйте чат поддержки в приложении

### Какое SLA для ответа на тикеты?

- **Critical** - 2 часа
- **High** - 8 часов
- **Medium** - 24 часа
- **Low** - 48 часов

### Есть ли документация для разработчиков?

Да, см. файлы в project/docs/:
- architecture.md - архитектура системы
- api-reference.md - описание API
- code-style.md - соглашения по коду

